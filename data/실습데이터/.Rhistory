m
as.matrix(dtm)
as.matrix(dtm)
m <- as.matrix(dtm)
m
colnames(m) <- c("듀크", "둘리", "또치", "도우너", "길동", "희동")
m
cps <- Corpus(VectorSource(fruit))
fruit
cps <- VCorpus(VectorSource(fruit))
cps
tdm <- TermDocumentMatrix(cps,
control=list(wordLengths=c(1,Inf)))
tdm
as.matrix(tdm)
cps <- Corpus(VectorSource(fruit))
cps
dtm <- DocumentTermMatrix(cps) # 가로가 doc 세로가 term
dtm
cps <- Corpus(VectorSource(fruit))
dtm <- DocumentTermMatrix(cps) # 가로가 doc 세로가 term
dtm
cps
dtm <- DocumentTermMatrix(cps) # 가로가 doc 세로가 term
dtm
tdm
as.matrix(tdm)
com <- m %*% t(m)
com
dist(com, method = "cosine")
# dist: distance
# cosine 각도
dist(com, method = "Euclidean")
cosine(com)
sort(cosine(com))
cosine(com)
str(cosine(com))
max(cosine(com))
unlist(cosine(com))
unlist(cosine(com)) - cosine(com)==1
unlist(cosine(com)) - cosine(com)
cosine(com)
qgraph(com, labels=rownames(com), diag=F,
layout='spring',  edge.color='blue',
vsize=log(diag(com)*800))
cosine(com)
cosine(com)==1 <- NA
cos <- cosine(com)
cos <- cos[!(cos==0)]
cos
cos <- cos[!(cos==1)]
cos
com
cos <- cosine(com)
cos
cos==1
cos[!(cos==1)]
cos==1 <- NA
cos==1 <- "NA"
cos
cos
gsub("1", "***", cos)
cos <- gsub("1", "***", cos)
cos <- gsub("1", "*", cos)
cos
cos <- cosine(com)
cos <- gsub("1", "*", cos)
cos
cos <- cosine(com)
cos
cos <- as.table(cosine(com))
cos
cos <- cos[!(cos==1)]
cos
cos <- as.table(cosine(com))
cos
cos <- cosine(com)
cos
cos <- as.list(cosine(com))
cos
cos <- cosine(com)
cos
install.pacakes("reshape2")
install.packages("reshape2")
library(reshape2)
head(french_fries)
mean(french_fries$potato)
french_fries$potato
mean(french_fries$potato)
french_fries %>% select(potato)
library(dplyr)
french_fries %>% select(potato)
french_fries %>% select(potato) %>% summarise(mean = mean())
french_fries %>% select(potato) %>% summarise(mean = mean(potato)
french_fries %>% select(potato) %>% summarise(mean <- mean(potato))
french_fries %>% select(potato) %>% summarise(mean())
french_fries %>% select(potato)
french_fries %>% select(potato) %>% summarise(mean = mean())
french_fries %>% select(potato) %>% summarise(mean = mean(potato))
str(french_fries)
french_fries %>% select(potato) %>% summarize(평균=mean(potato))
french_fries %>% select(potato) %>% summarize(평균=mean(potato), na.rm=T)
french_fries %>% select(potato) %>% summarize(평균=mean(), na.rm=T)
french_fries %>% select(potato) %>% summarize(평균=mean(potato), na.rm=T)
french_fries %>% select(potato) %>% mean()
mean(french_fries$potato, na.rm=T)
french_fries %>% select(potato) %>% summarise(평균= mean(), na.rm=T)
m %>% group_by(variable) %>% summarize(평균=mean(value, na.rm=T))
m %>% group_by(variable) %>% summarize(평균=mean(value, na.rm=T))
library(dplyr)
m %>% group_by(variable) %>% summarize(평균=mean(value, na.rm=T))
head(french_fries)
library(dplyr)
m %>% group_by(potato) %>% summarize(평균=mean(value, na.rm=T))
head(french_fries)
v
m %>% group_by(variable) %>% summarize(평균=mean(value, na.rm=T))
m2 <- melt(french_fries, id.vars=1:4, na.rm=T)
# 데이터열의 앞에서부터 네번째까지는 안바꾸겠다.
dim(m2)
dim(m)
m2 %>% group_by(variable) %>% summarize(평균=mean(value))
# reshape2: 데이터셋의 구조를 바꿔준다.
library(reshape2)
head(french_fries)
library(dplyr)
m %>% group_by(variable) %>% summarize(평균=mean(value, na.rm=T))
m2 <- melt(french_fries, id.vars=1:4, na.rm=T)
# 데이터열의 앞에서부터 네번째까지는 안바꾸겠다.
dim(m2)
dim(m)
m2 %>% group_by(variable) %>% summarize(평균=mean(value))
r <- dcast(m, time+treatment+subject + rep ~ ...)
# id 해당되는 이름을 엮는다. time~rep까지 사용하고 나머지 값을 갖다붙인다.
# 이때는 variable에 있는 애들이 변수값이 된다.
# decast; metl된 것을 다시 원래대로 되돌리겠다.
head(r)
rownames(r) <- NULL
rownames(french_fries) <- NULL
identical(r, french_fries)
?next
data(iris)
head(iris)
library(caret)
# 150건 --> 70%(105건 -->3)
idx = createDataPartition(iris$Species, p=0.7, list=F)
idx
iris_train = iris[idx,]
iris_train
iris_test = iris[-idx,]
iris_test
str(iris_train)
str(iris_test)
table(iris_train$Species)
table(iris_test$Species)
library(e1071)
install.packages("e1071")
install.packages("e1071")
install.packages("e1071")
library(e1071)
naiveBayes(Species ~., data=iris_train)
nv_model = naiveBayes(Species ~., data=iris_train)
predict(nv_model, iris_test, type='class')
nv_pred = predict(nv_model, iris_test, type='class')
confusionMatrix(nv_pred, iris_test$Species)
library(caret)
confusionMatrix(nv_pred, iris_test$Species)
naive.result = naiveBayes(iris_train, iris_train$Species, laplace=1)
naive.result
nv_model
naive.result = naiveBayes(iris_train, iris_train$Species, laplace=1)
naive.result
# 나이브베이즈 적합
navie.pred = predict(naive.result, iris_test, type='class')
table(naive.pred, iris_test$Species)
# 나이브베이즈 적합
# 테스트데이터 평가
navie.pred = predict(naive.result, iris_test, type='class')
table(naive.pred, iris_test$Species)
# 나이브베이즈 적합
# 테스트데이터 평가
naive.pred = predict(naive.result, iris_test, type='class')
table(naive.pred, iris_test$Species)
confusionMatrix(naive.pred, iris_test$Species)
table(naive.pred, iris_test$Species) # 분류결과 도출
?naiveBayes
confusionMatrix(naive.pred, iris_test$Species)
# 실전문제1
getwd()
setwd('c:/Statistics/data/실전데이터')
setwd('c:/Statistics/data/실습습데이터')
setwd('c:/Statistics/data/실습데이터')
read.csv('movie.csv', header=T)
movie = read.csv('movie.csv', header=T)
naiveBayes(movie[1:5], movie$장르, laplace=0)
movie
movie = read.csv('movie.csv', header=T)
naiveBayes(movie[1:5], movie$장르, laplace=0)
nm = naiveBayes(movie[1:5], movie$장르, laplace=0)
predict(nm, movie[1:5])
result = predict(nm, movie[1:5])
sum(movie$장르 != result)
result
# 실전문제2
read.csv('spam.csv', header=T)
# 실전문제2
read.csv('spam.csv', header=T)
# 실전문제2
spam = read.csv('spam.csv', header=T)
head(spam)
naiveBayes(spam[2:12], spam$메일종류, laplace=0)
nm2 = naiveBayes(spam[2:12], spam$메일종류, laplace=0)
result2 = predict(nm2, spam[2:12])
result2
sum(spam$메일종류 != result2)
result2
nm2
spam[is.na(spam)] = 0
nm2 = naiveBayes(spam[2:12], spam$메일종류, laplace=0)
result2 = predict(nm2, spam[2:12])
sum(spam$메일종류 != result2)
result2
nm2
result2
nm2 = naiveBayes(spam[2:12], spam$메일종류, laplace=0)
result2 = predict(nm2, spam[2:12])
result2
sum(spam$메일종류 != result2)
nm2 = naiveBayes(spam[2:13], spam$메일종류, laplace=0)
result2 = predict(nm2, spam[2:13])
nm2
# 인공신경망이론
library(caret)
idx = createDataPartition(iris$Species, p=0.7, list=F)
iris_train
iris_test
library(nnet)
iris_train_scale = as.data.frame(sapply(iris_test[,-5],scale))
iris_test_scale = as.data.frame(sapply(iris_test[,-5],scale))
# 데이터표준화
iris_train_scale = as.data.frame(sapply(iris_train[,-5],scale))
iris_test_scale = as.data.frame(sapply(iris_test[,-5],scale))
iris_train_scale$Species = iris_train$Species
iris_test_scale$Species = iris_test$Species
nnet.result = nnet(Species~., iris_train_scale, size=3)
# 훈련데이터 통한 모형 적합
nnet.pred = predict(nnet, result, iris_test_scale, type="class") # 테스트데이터 평가
library(e1071)
# 훈련데이터 통한 모형 적합
nnet.pred = predict(nnet, result, iris_test_scale, type="class") # 테스트데이터 평가
iris_train_scale = as.data.frame(sapply(iris_train[,-5],scale))
iris_test_scale = as.data.frame(sapply(iris_test[,-5],scale))
iris_train_scale$Species = iris_train$Species
iris_test_scale$Species = iris_test$Species
nnet.result = nnet(Species~., iris_train_scale, size=3)
# size는 노드 수
# 훈련데이터 통한 모형 적합
nnet.pred = predict(nnet, result, iris_test_scale, type="class") # 테스트데이터 평가
# 훈련데이터 통한 모형 적합
nnet.pred = predict(nnet.result, iris_test_scale, type="class") # 테스트데이터 평가
table(nnet.pred, iris_test$Species)
iris_train
# 데이터표준화
?scale
# 인공신경망 문제1
getwd()
read.csv('problem.csv'. header=T, stringAsFactors=F)
read.csv('problem.csv', header=T, stringAsFactors=F)
read.csv('problem.csv', header=T, stringAsFactors=F)
read.csv('problem.csv', header=T, stringsAsFactors = F)
prob = read.csv('problem.csv', header=T, stringsAsFactors = F)
head(prob)
?range
?diff
range(1,10)
diff(range(1,10))
#정규화
function(x){
return((x-min(x)) / diff(range(x)))
}
#정규화
normalize = function(x){
return((x-min(x)) / diff(range(x)))
}
normalize
?with
# diff(): 연속 값 간의 차이
prob$accident2 = with(prob, ifelse(accident=="suicide" | accident=="violence", 1, 0))
head(prob)
prob[31]
prob = prob[-31]
normalize(prob)
prob
normalize(prob[1:30])
prob[1:30] = normalize(prob[1:30])
head(prob)
m1 = nnet(accident2 ~., data=prob, size=10)
r1 = predict(m1, prob)
r1
r1 = predict(m1, prob)
r1
head(r1)
cbind(prob$accident2, r1>0.5)
sum(as.numeric(r1>0.5) != prob$accident2)
table(nnet.pred, iris_test$Species) # 분류결과 도출
#스케일링 먼저 해주고 train, test로 나누는 게 편하다?
iris_train_scale = as.data.frame(sapply(iris_train[,-5],scale))
iris_test_scale = as.data.frame(sapply(iris_test[,-5],scale))
iris_train_scale$Species = iris_train$Species
nnet.result = nnet(Species~., iris_train_scale, size=3)
nnet.result
nnet.result
# 훈련데이터 통한 모형 적합
nnet.pred = predict(nnet.result, iris_test_scale, type="class") # 테스트데이터 평가
table(nnet.pred, iris_test$Species) # 분류결과 도출
sum(movie$장르 != result)
head(prob)
m1 = nnet(accident2 ~., data=prob, size=10)
m1
nnet.result = nnet(Species~., iris_train_scale, size=3)
nnet.result
m1 = nnet(accident2 ~., data=prob, size=10)
m1
r1 = predict(m1, prob)
r1
# 훈련데이터 통한 모형 적합
nnet.pred = predict(nnet.result, iris_test_scale, type="class") # 테스트데이터 평가
nnet.pred
table(nnet.pred, iris_test$Species) # 분류결과 도출
prob = read.csv('problem.csv', header=T, stringsAsFactors = F)
#정규화
normalize = function(x){
return((x-min(x)) / diff(range(x)))
}
prob[1:30] = normalize(prob[1:30])
head(prob)
# diff(): 연속 값 간의 차이
prob$accident2 = with(prob, ifelse(accident=="suicide" | accident=="violence", 1, 0))
head(prob)
prob = prob[-31]
prob
head(prob)
m1 = nnet(accident2 ~., data=prob, size=10)
r1 = predict(m1, prob)
r1
m1
prob
prob = read.csv('problem.csv', header=T, stringsAsFactors = F)
prob
#정규화
normalize = function(x){
return((x-min(x)) / diff(range(x)))
}
prob[1:30] = normalize(prob[1:30])
# diff(): 연속 값 간의 차이
prob$accident2 = with(prob, ifelse(accident=="suicide" | accident=="violence", 1, 0))
prob = prob[-31]
prob
head(prob)
r1 = predict(m1, prob)
r1
cbind(prob$accident2, r1>0.5)
prob
cbind(prob$accident2, r1>0.5)
prob = read.csv('problem.csv', header=T, stringsAsFactors = F)
#정규화
normalize = function(x){
return((x-min(x)) / diff(range(x)))
}
prob[1:30] = normalize(prob[1:30])
head(prob)
prob = prob[-31]
prob = read.csv('problem.csv', header=T, stringsAsFactors = F)
#정규화
normalize = function(x){
return((x-min(x)) / diff(range(x)))
}
prob[1:30] = normalize(prob[1:30])
# diff(): 연속 값 간의 차이
prob$accident2 = with(prob, ifelse(accident=="suicide" | accident=="violence", 1, 0))
prob = prob[-31]
head(prob)
m1 = nnet(accident2 ~., data=prob, size=10)
r1 = predict(m1, prob)
str(r1)
r1 = predict(m1, prob)
r1
prob = read.csv('problem.csv', header=T, stringsAsFactors = F)
#정규화
normalize = function(x){
return((x-min(x)) / diff(range(x)))
}
prob[1:30] = normalize(prob[1:30])
# diff(): 연속 값 간의 차이
prob$accident2 = with(prob, ifelse(accident=="suicide" | accident=="violence", 1, 0))
prob = prob[-31]
m1 = nnet(accident2 ~., data=prob, size=10)
r1 = predict(m1, prob)
r1
nnet.pred
table(nnet.pred, iris_test$Species)
cbind(prob$accident2, r1>0.5)
sum(as.numeric(r1>0.5) != prob$accident2)
prob = read.csv('problem.csv', header=T, stringsAsFactors = F)
#정규화
normalize = function(x){
return((x-min(x)) / diff(range(x)))
}
prob[1:30] = normalize(prob[1:30])
# diff(): 연속 값 간의 차이
prob$accident2 = with(prob, ifelse(accident=="suicide" | accident=="violence", 1, 0))
prob = prob[-31]
m1 = nnet(accident2 ~., data=prob, size=10)
r1 = predict(m1, prob)
cbind(prob$accident2, r1>0.5)
sum(as.numeric(r1>0.5) != prob$accident2)
# 같은 방법 다른 패키지
library(neuralnet)
# 같은 방법 다른 패키지
install.packages("neuralnet")
library(neuralnet)
xnam = paste("ans", 1:30)
xnam
fmla = as.formula(paste("accident2 ~", paste(xnam, collapse="+")))
prob
m1 = nnet(accident2 ~., data=prob, size=10)
m1
r1 = predict(m1, prob)
r1
str(r1)
r1
head(r1)
cbind(prob$accident2, r1>0.5)
prob$accident2
cbind(prob$accident2, r1>0.5)
prob$accident2
r1>0.5
cbind(prob$accident2, r1>0.5)
sum(as.numeric(r1>0.5) != prob$accident2)
library(neuralnet)
xnam = paste("ans", 1:30)
xnam
fmla = as.formula(paste("accident2 ~", paste(xnam, collapse="+")))
fmla = as.formula(paste("accident2 ~ ", paste(xnam, collapse= "+")))
fmla
as.formula(paste("accident2 ~ ", paste(xnam, collapse= "+")))
xnam
str(xnam)
prob
xnam = paste("ans", 1:30, sep="")
xnam
str(xnam)
as.formula(paste("accident2 ~ ", paste(xnam, collapse= "+")))
fmla = as.formula(paste("accident2 ~ ", paste(xnam, collapse= "+")))
fmla
?as.formula
# formula(): 처리할 데이터를 마치 수식처럼 사용하는 함수
m2 = neuralnet(fmla, data=prob, hidden=10)
m2
fmla
plot(m2)
?neuralnet
plot(m2)
?naiveBayes
naive.result
iris_train
naive.result = naiveBayes(iris_train, iris_train$Species, laplace=0)
naive.result
naive.result = naiveBayes(iris_train, iris_train$Species, laplace=1)
naive.result
# 나이브베이즈 적합
# 테스트데이터 평가
naive.pred = predict(naive.result, iris_test, type='class')
naive.pred
naive.result
table(naive.pred, iris_test$Species) # 분류결과 도출
confusionMatrix(naive.pred, iris_test$Species)
iris_train
naive.result = naiveBayes(iris_train, iris_train$Species, laplace=1)
naive.result
movie = read.csv('movie.csv', header=T)
movie
nm = naiveBayes(movie, movie$장르, laplace=0)
nm
nm = naiveBayes(movie[1:5], movie$장르, laplace=0)
nm
naive.result = naiveBayes(iris_train, iris_train$Species, laplace=1)
naive.result
# 나이브베이즈 적합
# 테스트데이터 평가
naive.pred = predict(naive.result, iris_test, type='class')
naive.pred
iris_train
nm = naiveBayes(movie, movie$장르, laplace=0)
nm
nm = naiveBayes(movie[1:5], movie$장르, laplace=0)
nm
# 나이브베이즈 적합
# 테스트데이터 평가
naive.pred = predict(naive.result, iris_test, type='class')
naive.pred
naive.result
table(naive.pred, iris_test$Species) # 분류결과 도출
nm = naiveBayes(movie, movie$장르, laplace=0)
nm
result = predict(nm, movie[1:5])
result
sum(movie$장르 != result)
nm = naiveBayes(movie[1:5], movie$장르, laplace=0)
result = predict(nm, movie[1:5])
sum(movie$장르 != result)
plot(m2)
